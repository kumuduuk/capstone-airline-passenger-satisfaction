{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578d67ed",
   "metadata": {},
   "source": [
    "## 1. Loading the Dataset\n",
    "\n",
    "This step reads the raw training and testing data into Pandas DataFrames, which are the fundamental structures for data manipulation in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df19f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('..\\\\data\\\\raw_data\\\\train.csv')\n",
    "test_df = pd.read_csv('..\\\\data\\\\raw_data\\\\test.csv')\n",
    "\n",
    "# Combine datasets for consistent preprocessing.\n",
    "# I'll keep the distinction with an 'is_train' column for later splitting\n",
    "train_df['is_train'] = 1\n",
    "test_df['is_train'] = 0\n",
    "combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "print(\"DataFrames loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d3e7af",
   "metadata": {},
   "source": [
    "### 2. Initial Data Exploration\n",
    "\n",
    "Understanding the dataset's composition is crucial to planning the data cleaning strategy. Inspect the first few rows, check the overall dimensions, and review the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a7ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Combined DataFrame Head (First 5 Rows) ---\")\n",
    "print(combined_df.head())\n",
    "\n",
    "print(\"\\n--- Combined DataFrame Shape ---\")\n",
    "print(f\"Rows: {combined_df.shape[0]}, Columns: {combined_df.shape[1]}\")\n",
    "\n",
    "print(\"\\n--- Combined DataFrame Info (Data Types & Non-Null Counts) ---\")\n",
    "print(combined_df.info())\n",
    "\n",
    "print(\"\\n--- Descriptive Statistics (Numerical Columns) ---\")\n",
    "print(combined_df.describe().T)\n",
    "\n",
    "# Check unique values in categorical columns to spot issues\n",
    "print(\"\\n--- Unique Values in Categorical Columns ---\")\n",
    "for col in ['Gender', 'Customer Type', 'Type of Travel', 'Class', 'satisfaction']:\n",
    "    print(f\"{col}: {combined_df[col].unique()}\")\n",
    "\n",
    "# Check for unexpected negative values in numerical columns (e.g., delays, distance, age)\n",
    "# Age, Flight Distance, Departure Delay in Minutes, Arrival Delay in Minutes should all be >= 0.\n",
    "numerical_check = combined_df[['Age', 'Flight Distance', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']].min()\n",
    "print(\"\\n--- Minimum Values Check (Looking for unexpected negatives) ---\")\n",
    "print(numerical_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f125c47",
   "metadata": {},
   "source": [
    "### 3. Handling Missing/Invalid Values\n",
    "\n",
    "Missing data can skew the analysis and break models. It's essential to identify and address nulls and other obvious inconsistencies (like duplicates) early on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ebd451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in the combined dataset\n",
    "print(\"--- Missing Values Count ---\")\n",
    "missing_values = combined_df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8c1019",
   "metadata": {},
   "source": [
    "#### Handling Missing Values\n",
    "\n",
    "1. **`Arrival Delay in Minutes`**  \n",
    "   This is the only column with missing values in the dataset, accounting for approximately **0.3%** of the total records.  \n",
    "   To determine an appropriate imputation strategy, the distribution of arrival delays will be examined using a histogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d39a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_data = combined_df[\"Arrival Delay in Minutes\"].dropna() # Get only values without NANs.\n",
    "\n",
    "# Calculate the mean, median and mode\n",
    "data_mean = delay_data.mean()\n",
    "data_median = delay_data.median()\n",
    "data_mode = delay_data.mode().iloc[0] # take the first mode if multiple exist\n",
    "\n",
    "# Draw histogram\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(\n",
    "    delay_data,\n",
    "    bins=200,\n",
    "    density=False,\n",
    "    alpha=0.7,\n",
    "    color='skyblue',\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "# Add vertical lines for the statistics\n",
    "plt.axvline(data_mean, color='red', linestyle='dashed', linewidth=0.5, \n",
    "            label=f'Mean: {data_mean:.2f} mins')\n",
    "plt.axvline(data_median, color='green', linestyle='dashed', linewidth=0.5, \n",
    "            label=f'Median: {data_median:.2f} mins')\n",
    "plt.axvline(data_mode, color='purple', linestyle='dashed', linewidth=0.5, \n",
    "            label=f'Mode: {data_mode:.2f} mins')\n",
    "\n",
    "# Finalize the plot\n",
    "plt.xlabel(\"Arrival Delay in Minutes\")\n",
    "plt.ylabel(\"Count of Flights\")\n",
    "plt.title(f'Distribution of Arrival Delays ')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "print(f\"Mean: {data_mean} mins\")\n",
    "print(f\"Median: {data_median} mins\")\n",
    "print(f\"Mode: {data_mode} mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26cf50b",
   "metadata": {},
   "source": [
    "- **Mean:** 15.07 minutes  \n",
    "- **Median:** 0.0 minutes  \n",
    "- **Mode:** 0.0 minutes  \n",
    "\n",
    "The histogram shows a **strong right-skewed distribution**, indicating that most flights are on time or experience minimal delays, while a smaller proportion of flights have large delays. Since both the **median and mode are 0**, missing values in the arrival delay column are **imputed with 0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2be33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces all NaN values in Arrival Delay in Minutes with 0\n",
    "combined_df[\"Arrival Delay in Minutes\"] = combined_df[\"Arrival Delay in Minutes\"].fillna(0)\n",
    "print(\"Replace all NAN values in Arrival Delay in Minutes with zero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b989cb9",
   "metadata": {},
   "source": [
    "#### Handling duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78956349",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Handling Duplicates ---\n",
    "\n",
    "# Check for and drop duplicate rows (keeping the first occurrence)\n",
    "print(f\"\\nNumber of duplicate rows before dropping: {combined_df.duplicated().sum()}\")\n",
    "combined_df.drop_duplicates(inplace=True)\n",
    "print(f\"Number of rows after dropping duplicates: {combined_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9352445",
   "metadata": {},
   "source": [
    "### 4. Renaming Columns\n",
    "\n",
    "Standardizing column names (e.g., using snake_case and removing spaces/special characters) improves readability and makes coding easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c239f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for renaming columns to snake_case\n",
    "new_columns = {\n",
    "    'Customer Type': 'Customer_Type',\n",
    "    'Type of Travel': 'Type_of_Travel',\n",
    "    'Flight Distance': 'Flight_Distance',\n",
    "    'Inflight wifi service': 'Inflight_wifi_service',\n",
    "    'Departure/Arrival time convenient': 'Dep_Arr_Time_Convenient',\n",
    "    'Ease of Online booking': 'Ease_of_Online_booking',\n",
    "    'Gate location': 'Gate_Location',\n",
    "    'Food and drink': 'Food_and_Drink',\n",
    "    'Online boarding': 'Online_Boarding',\n",
    "    'Seat comfort': 'Seat_Comfort',\n",
    "    'Inflight entertainment': 'Inflight_Entertainment',\n",
    "    'On-board service': 'On_board_Service',\n",
    "    'Leg room service': 'Leg_Room_Service',\n",
    "    'Baggage handling': 'Baggage_Handling',\n",
    "    'Checkin service': 'Checkin_Service',\n",
    "    'Inflight service': 'Inflight_Service',\n",
    "    'Departure Delay in Minutes': 'Departure_Delay_in_Minutes',\n",
    "    'Arrival Delay in Minutes': 'Arrival_Delay_in_Minutes'\n",
    "}\n",
    "\n",
    "combined_df.rename(columns=new_columns, inplace=True)\n",
    "\n",
    "print(\"--- Sample of Renamed Columns ---\")\n",
    "print(combined_df.columns[1:10])\n",
    "\n",
    "# Identify and remove the redundant index columns\n",
    "# 'Unnamed: 0' is a remnant of an old index saved to CSV.\n",
    "# 'id' is a unique identifier, not useful for modeling.\n",
    "columns_to_drop = ['Unnamed: 0', 'id']\n",
    "\n",
    "# Use errors='ignore' in case the column was already dropped in a previous run\n",
    "combined_df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "print(\"Redundant columns 'Unnamed: 0' and 'id' have been successfully removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099f2a14",
   "metadata": {},
   "source": [
    "### 5. Mapping Categorical Values\n",
    "\n",
    "Some categorical variables are currently represented by numerical codes (0-5 scale for service ratings) or verbose strings. I'll map the target variable and address the numeric service ratings by ensuring they are recognized as categorical if a simple ordinal scale isn't sufficient for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3488468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mapping Target Variable ('satisfaction')  ---\n",
    "\n",
    "# Map target variable to numerical (1 for satisfied, 0 for dissatisfied/neutral)\n",
    "satisfaction_mapping = {\n",
    "    'satisfied': 1,\n",
    "    'neutral or dissatisfied': 0\n",
    "}\n",
    "combined_df['satisfaction'] = combined_df['satisfaction'].map(satisfaction_mapping)\n",
    "print(\"\\n'satisfaction' mapped to binary (1=satisfied, 0=dissatisfied).\")\n",
    "print(combined_df['satisfaction'].head())\n",
    "\n",
    "# --- Handling Ordinal/Categorical Columns (Service Ratings) ---\n",
    "# Service rating columns are ordinal (1â€“5), so they are kept as integers.\n",
    "\n",
    "rating_cols = ['Inflight_wifi_service', 'Dep_Arr_Time_Convenient', 'Ease_of_Online_booking', \n",
    "               'Gate_Location', 'Food_and_Drink', 'Online_Boarding', 'Seat_Comfort', \n",
    "               'Inflight_Entertainment', 'On_board_Service', 'Leg_Room_Service', \n",
    "               'Baggage_Handling', 'Checkin_Service', 'Inflight_Service', 'Cleanliness']\n",
    "\n",
    "# Convert to appropriate types (e.g., integer for rating, string for object types)\n",
    "for col in rating_cols:\n",
    "    combined_df[col] = combined_df[col].astype('int') \n",
    "\n",
    "# Convert other object columns to category type for efficiency\n",
    "for col in ['Gender', 'Customer_Type', 'Type_of_Travel', 'Class']:\n",
    "    combined_df[col] = combined_df[col].astype('category')\n",
    "    \n",
    "print(\"\\nService rating columns coerced to integer. Key nominal columns converted to 'category' Dtype.\")\n",
    "print(combined_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c46077",
   "metadata": {},
   "source": [
    "### 6. Converting Data Types\n",
    "\n",
    "This step ensures all columns have the most memory-efficient and appropriate data types before starting analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfa5b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing this for optimize the memory.\n",
    "columns_list = ['Age', 'Flight_Distance', 'Departure_Delay_in_Minutes', 'Arrival_Delay_in_Minutes']\n",
    "for col in columns_list:\n",
    "    # Use 'Int64' to handle any remaining nullable integers gracefully, though I filled NaNs\n",
    "    combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce').round(0).astype('Int64')\n",
    "\n",
    "# Here i used \"errors='coerce'\", so there can be NaNs due to unexpected characters. So check the NaNs again for safety.\n",
    "print(\"Checking for NaNs introduced during the type change\")\n",
    "for col in columns_list:\n",
    "    print(f\"{col} NaN count: {combined_df[col].isnull().sum()}\")\n",
    "          \n",
    "print(\"\\n--- Final Data Types Check (Post-Conversion) ---\")\n",
    "print(combined_df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0198f8c6",
   "metadata": {},
   "source": [
    "### 7. Saving Cleaned Data\n",
    "\n",
    "The final step is saving your cleaned data so you don't have to run the initial cleaning every time you return to the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c955610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the processed combined data back into train and test sets\n",
    "train_processed = combined_df[combined_df['is_train'] == 1].drop(columns=['is_train']).copy()\n",
    "test_processed = combined_df[combined_df['is_train'] == 0].drop(columns=['is_train']).copy()\n",
    "\n",
    "# Save the cleaned data to new CSV files\n",
    "train_processed.to_csv('..\\\\data\\\\cleaned_data\\\\train_cleaned.csv', index=False)\n",
    "test_processed.to_csv('..\\\\data\\\\cleaned_data\\\\test_cleaned.csv', index=False)\n",
    "\n",
    "print(\"\\nCleaned datasets saved as 'train_cleaned.csv' and 'test_cleaned.csv'.\")\n",
    "print(\"\\n--- Final Cleaned Training Data Head ---\")\n",
    "print(train_processed.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
